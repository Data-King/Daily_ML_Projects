"""
Daily Stock Movement Classifier (Up/Down)
Binary classification using OHLCV features
Step-by-step implementation with sample data
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                             f1_score, confusion_matrix, classification_report, roc_auc_score, roc_curve)
import warnings
warnings.filterwarnings('ignore')

# Set style for visualizations
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("="*70)
print("DAILY STOCK MOVEMENT CLASSIFIER PROJECT")
print("Predict if stock will go UP ‚Üë or DOWN ‚Üì")
print("="*70)

# ============================================================================
# STEP 1: GENERATE SAMPLE OHLCV DATA
# ============================================================================
print("\n[STEP 1] Generating Sample Stock Market Data (OHLCV)...")

np.random.seed(42)
n_days = 1000

# Generate realistic stock price data
dates = pd.date_range(start='2022-01-01', periods=n_days, freq='D')
initial_price = 100

# Simulate price movements with trend and volatility
returns = np.random.normal(0.0005, 0.02, n_days)  # Mean return 0.05%, volatility 2%
trend = np.linspace(0, 0.3, n_days)  # Slight upward trend
returns = returns + trend / n_days

prices = initial_price * np.exp(np.cumsum(returns))

# Generate OHLCV data
data = {
    'Date': dates,
    'Open': prices * (1 + np.random.uniform(-0.01, 0.01, n_days)),
    'High': prices * (1 + np.random.uniform(0, 0.03, n_days)),
    'Low': prices * (1 + np.random.uniform(-0.03, 0, n_days)),
    'Close': prices,
    'Volume': np.random.randint(1000000, 10000000, n_days)
}

df = pd.DataFrame(data)

# Ensure High is highest and Low is lowest
df['High'] = df[['Open', 'High', 'Close']].max(axis=1)
df['Low'] = df[['Open', 'Low', 'Close']].min(axis=1)

print(f"‚úì Generated {len(df)} days of OHLCV stock data")
print(f"\nFirst 10 rows of data:")
print(df.head(10))
print(f"\nData Statistics:")
print(df.describe())

# ============================================================================
# STEP 2: FEATURE ENGINEERING
# ============================================================================
print("\n" + "="*70)
print("[STEP 2] Feature Engineering - Creating Technical Indicators...")

# Price-based features
df['Daily_Return'] = df['Close'].pct_change()
df['Price_Range'] = df['High'] - df['Low']
df['Price_Change'] = df['Close'] - df['Open']
df['Gap'] = df['Open'] - df['Close'].shift(1)

# Moving Averages
df['SMA_5'] = df['Close'].rolling(window=5).mean()
df['SMA_10'] = df['Close'].rolling(window=10).mean()
df['SMA_20'] = df['Close'].rolling(window=20).mean()
df['EMA_12'] = df['Close'].ewm(span=12, adjust=False).mean()
df['EMA_26'] = df['Close'].ewm(span=26, adjust=False).mean()

# MACD (Moving Average Convergence Divergence)
df['MACD'] = df['EMA_12'] - df['EMA_26']
df['MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()
df['MACD_Hist'] = df['MACD'] - df['MACD_Signal']

# RSI (Relative Strength Index)
delta = df['Close'].diff()
gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
rs = gain / loss
df['RSI'] = 100 - (100 / (1 + rs))

# Bollinger Bands
df['BB_Middle'] = df['Close'].rolling(window=20).mean()
bb_std = df['Close'].rolling(window=20).std()
df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)
df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)
df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']
df['BB_Position'] = (df['Close'] - df['BB_Lower']) / df['BB_Width']

# Volume features
df['Volume_Change'] = df['Volume'].pct_change()
df['Volume_MA'] = df['Volume'].rolling(window=20).mean()
df['Volume_Ratio'] = df['Volume'] / df['Volume_MA']

# Momentum indicators
df['Momentum'] = df['Close'] - df['Close'].shift(4)
df['ROC'] = ((df['Close'] - df['Close'].shift(10)) / df['Close'].shift(10)) * 100

# Volatility
df['Volatility'] = df['Daily_Return'].rolling(window=10).std()

# Price position relative to moving averages
df['Price_to_SMA5'] = df['Close'] / df['SMA_5']
df['Price_to_SMA20'] = df['Close'] / df['SMA_20']

# Lagged features
df['Close_Lag1'] = df['Close'].shift(1)
df['Close_Lag2'] = df['Close'].shift(2)
df['Volume_Lag1'] = df['Volume'].shift(1)

print("‚úì Created technical indicators:")
print("  ‚Ä¢ Moving Averages (SMA, EMA)")
print("  ‚Ä¢ MACD and Signal lines")
print("  ‚Ä¢ RSI (Relative Strength Index)")
print("  ‚Ä¢ Bollinger Bands")
print("  ‚Ä¢ Volume indicators")
print("  ‚Ä¢ Momentum and ROC")
print("  ‚Ä¢ Volatility measures")

# ============================================================================
# STEP 3: CREATE TARGET VARIABLE
# ============================================================================
print("\n" + "="*70)
print("[STEP 3] Creating Target Variable (Up/Down Classification)...")

# Target: 1 if tomorrow's close is higher than today's, 0 otherwise
df['Tomorrow_Close'] = df['Close'].shift(-1)
df['Target'] = (df['Tomorrow_Close'] > df['Close']).astype(int)

# Drop rows with NaN values
df = df.dropna()

up_days = df['Target'].sum()
down_days = len(df) - up_days

print(f"‚úì Target variable created:")
print(f"  ‚Ä¢ UP days (1): {up_days} ({up_days/len(df)*100:.1f}%)")
print(f"  ‚Ä¢ DOWN days (0): {down_days} ({down_days/len(df)*100:.1f}%)")
print(f"  ‚Ä¢ Total valid samples: {len(df)}")

# ============================================================================
# STEP 4: PREPARE FEATURES AND TARGET
# ============================================================================
print("\n" + "="*70)
print("[STEP 4] Preparing Features for Model Training...")

# Select features for modeling (exclude non-predictive columns)
feature_columns = [
    'Daily_Return', 'Price_Range', 'Price_Change', 'Gap',
    'SMA_5', 'SMA_10', 'SMA_20', 'EMA_12', 'EMA_26',
    'MACD', 'MACD_Signal', 'MACD_Hist',
    'RSI', 'BB_Width', 'BB_Position',
    'Volume_Change', 'Volume_Ratio',
    'Momentum', 'ROC', 'Volatility',
    'Price_to_SMA5', 'Price_to_SMA20',
    'Close_Lag1', 'Close_Lag2', 'Volume_Lag1'
]

X = df[feature_columns]
y = df['Target']

print(f"‚úì Selected {len(feature_columns)} features")
print(f"‚úì Target variable: Binary (0=Down, 1=Up)")
print(f"‚úì Total samples ready: {len(X)}")

# ============================================================================
# STEP 5: TRAIN-TEST SPLIT (TIME SERIES SPLIT)
# ============================================================================
print("\n" + "="*70)
print("[STEP 5] Splitting Data (Time Series Split - Chronological)...")

# For time series, we use chronological split (not random)
split_idx = int(len(df) * 0.8)
X_train = X.iloc[:split_idx]
X_test = X.iloc[split_idx:]
y_train = y.iloc[:split_idx]
y_test = y.iloc[split_idx:]

print(f"‚úì Training set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)")
print(f"  - Date range: {df.iloc[:split_idx]['Date'].min().date()} to {df.iloc[:split_idx]['Date'].max().date()}")
print(f"‚úì Testing set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)")
print(f"  - Date range: {df.iloc[split_idx:]['Date'].min().date()} to {df.iloc[split_idx:]['Date'].max().date()}")

# ============================================================================
# STEP 6: FEATURE SCALING
# ============================================================================
print("\n" + "="*70)
print("[STEP 6] Scaling Features...")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("‚úì Features normalized using StandardScaler")
print("‚úì Mean = 0, Standard Deviation = 1")

# ============================================================================
# STEP 7: MODEL TRAINING
# ============================================================================
print("\n" + "="*70)
print("[STEP 7] Training Multiple Classification Models...")

models = {}

# Model 1: Logistic Regression
print("\n1. Training Logistic Regression...")
lr_model = LogisticRegression(random_state=42, max_iter=1000)
lr_model.fit(X_train_scaled, y_train)
models['Logistic Regression'] = lr_model
print("‚úì Logistic Regression trained")

# Model 2: Random Forest Classifier
print("\n2. Training Random Forest Classifier...")
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf_model.fit(X_train_scaled, y_train)
models['Random Forest'] = rf_model
print("‚úì Random Forest trained")

# Model 3: Gradient Boosting Classifier
print("\n3. Training Gradient Boosting Classifier...")
gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)
gb_model.fit(X_train_scaled, y_train)
models['Gradient Boosting'] = gb_model
print("‚úì Gradient Boosting trained")

# ============================================================================
# STEP 8: MODEL EVALUATION
# ============================================================================
print("\n" + "="*70)
print("[STEP 8] Evaluating Model Performance...")

results = {}

for name, model in models.items():
    # Predictions
    y_train_pred = model.predict(X_train_scaled)
    y_test_pred = model.predict(X_test_scaled)
    y_test_proba = model.predict_proba(X_test_scaled)[:, 1]
    
    # Calculate metrics
    train_acc = accuracy_score(y_train, y_train_pred)
    test_acc = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    recall = recall_score(y_test, y_test_pred)
    f1 = f1_score(y_test, y_test_pred)
    roc_auc = roc_auc_score(y_test, y_test_proba)
    
    results[name] = {
        'train_acc': train_acc,
        'test_acc': test_acc,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'roc_auc': roc_auc,
        'y_pred': y_test_pred,
        'y_proba': y_test_proba
    }
    
    print(f"\n{name}:")
    print(f"  Training Accuracy:   {train_acc:.4f} ({train_acc*100:.2f}%)")
    print(f"  Testing Accuracy:    {test_acc:.4f} ({test_acc*100:.2f}%)")
    print(f"  Precision:           {precision:.4f}")
    print(f"  Recall:              {recall:.4f}")
    print(f"  F1-Score:            {f1:.4f}")
    print(f"  ROC-AUC Score:       {roc_auc:.4f}")

# Find best model
best_model_name = max(results, key=lambda x: results[x]['test_acc'])
best_model = models[best_model_name]
print(f"\nüèÜ Best Model: {best_model_name} (Accuracy: {results[best_model_name]['test_acc']*100:.2f}%)")

# ============================================================================
# STEP 9: DETAILED ANALYSIS OF BEST MODEL
# ============================================================================
print("\n" + "="*70)
print(f"[STEP 9] Detailed Analysis of {best_model_name}...")

# Confusion Matrix
cm = confusion_matrix(y_test, results[best_model_name]['y_pred'])
print("\nConfusion Matrix:")
print(f"                 Predicted Down  Predicted Up")
print(f"Actual Down           {cm[0][0]}             {cm[0][1]}")
print(f"Actual Up             {cm[1][0]}             {cm[1][1]}")

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, results[best_model_name]['y_pred'], 
                          target_names=['DOWN ‚Üì', 'UP ‚Üë']))

# Feature Importance (for tree-based models)
if best_model_name in ['Random Forest', 'Gradient Boosting']:
    feature_importance = pd.DataFrame({
        'Feature': feature_columns,
        'Importance': best_model.feature_importances_
    }).sort_values('Importance', ascending=False)
    
    print("\nTop 10 Most Important Features:")
    for idx, row in feature_importance.head(10).iterrows():
        print(f"  {row['Feature']:.<30} {row['Importance']:.6f}")

# ============================================================================
# STEP 10: PREDICTION EXAMPLES
# ============================================================================
print("\n" + "="*70)
print("[STEP 10] Making Predictions on Recent Data...")

# Get last 5 days from test set
recent_data = X_test.tail(5)
recent_dates = df.iloc[split_idx:].tail(5)['Date']
recent_scaled = scaler.transform(recent_data)
predictions = best_model.predict(recent_scaled)
probabilities = best_model.predict_proba(recent_scaled)

print("\nRecent Predictions:")
print("-" * 70)
for i, (date, pred, proba) in enumerate(zip(recent_dates, predictions, probabilities)):
    direction = "UP ‚Üë" if pred == 1 else "DOWN ‚Üì"
    confidence = proba[pred] * 100
    actual = "UP ‚Üë" if y_test.iloc[-(5-i)] == 1 else "DOWN ‚Üì"
    correct = "‚úì" if pred == y_test.iloc[-(5-i)] else "‚úó"
    
    print(f"Date: {date.date()}")
    print(f"  Prediction: {direction} (Confidence: {confidence:.1f}%) {correct}")
    print(f"  Actual: {actual}")
    print()

# ============================================================================
# STEP 11: VISUALIZATIONS
# ============================================================================
print("\n" + "="*70)
print("[STEP 11] Creating Comprehensive Visualizations...")

fig = plt.figure(figsize=(18, 12))

# Plot 1: Stock Price with Moving Averages
ax1 = plt.subplot(3, 3, 1)
plt.plot(df['Date'].iloc[-200:], df['Close'].iloc[-200:], label='Close Price', linewidth=2)
plt.plot(df['Date'].iloc[-200:], df['SMA_20'].iloc[-200:], label='SMA 20', alpha=0.7)
plt.plot(df['Date'].iloc[-200:], df['SMA_5'].iloc[-200:], label='SMA 5', alpha=0.7)
plt.xlabel('Date')
plt.ylabel('Price ($)')
plt.title('Stock Price with Moving Averages', fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)

# Plot 2: Model Accuracy Comparison
ax2 = plt.subplot(3, 3, 2)
model_names = list(results.keys())
accuracies = [results[m]['test_acc'] for m in model_names]
colors = ['#3498db', '#2ecc71', '#e74c3c']
bars = plt.bar(model_names, accuracies, color=colors, alpha=0.7, edgecolor='black')
plt.ylabel('Accuracy')
plt.title('Model Performance Comparison', fontweight='bold')
plt.ylim([0, 1])
for bar, acc in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
             f'{acc:.3f}', ha='center', fontweight='bold')
plt.xticks(rotation=15)
plt.grid(True, alpha=0.3, axis='y')

# Plot 3: Confusion Matrix Heatmap
ax3 = plt.subplot(3, 3, 3)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['DOWN', 'UP'], yticklabels=['DOWN', 'UP'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title(f'Confusion Matrix - {best_model_name}', fontweight='bold')

# Plot 4: ROC Curves
ax4 = plt.subplot(3, 3, 4)
for name, res in results.items():
    fpr, tpr, _ = roc_curve(y_test, res['y_proba'])
    plt.plot(fpr, tpr, label=f"{name} (AUC={res['roc_auc']:.3f})", linewidth=2)
plt.plot([0, 1], [0, 1], 'k--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves', fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)

# Plot 5: Feature Importance (Top 10)
if best_model_name in ['Random Forest', 'Gradient Boosting']:
    ax5 = plt.subplot(3, 3, 5)
    top_features = feature_importance.head(10)
    plt.barh(top_features['Feature'], top_features['Importance'])
    plt.xlabel('Importance')
    plt.title('Top 10 Feature Importance', fontweight='bold')
    plt.gca().invert_yaxis()

# Plot 6: RSI Indicator
ax6 = plt.subplot(3, 3, 6)
plt.plot(df['Date'].iloc[-200:], df['RSI'].iloc[-200:], label='RSI', linewidth=2)
plt.axhline(y=70, color='r', linestyle='--', label='Overbought (70)')
plt.axhline(y=30, color='g', linestyle='--', label='Oversold (30)')
plt.xlabel('Date')
plt.ylabel('RSI')
plt.title('Relative Strength Index (RSI)', fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)

# Plot 7: MACD
ax7 = plt.subplot(3, 3, 7)
plt.plot(df['Date'].iloc[-200:], df['MACD'].iloc[-200:], label='MACD', linewidth=2)
plt.plot(df['Date'].iloc[-200:], df['MACD_Signal'].iloc[-200:], label='Signal', linewidth=2)
plt.bar(df['Date'].iloc[-200:], df['MACD_Hist'].iloc[-200:], label='Histogram', alpha=0.3)
plt.xlabel('Date')
plt.ylabel('MACD')
plt.title('MACD Indicator', fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(rotation=45)

# Plot 8: Volume Analysis
ax8 = plt.subplot(3, 3, 8)
colors_vol = ['red' if df['Close'].iloc[i] < df['Open'].iloc[i] else 'green' 
              for i in range(-200, 0)]
plt.bar(df['Date'].iloc[-200:], df['Volume'].iloc[-200:], color=colors_vol, alpha=0.6)
plt.plot(df['Date'].iloc[-200:], df['Volume_MA'].iloc[-200:], color='blue', 
         linewidth=2, label='Volume MA')
plt.xlabel('Date')
plt.ylabel('Volume')
plt.title('Trading Volume', fontweight='bold')
plt.legend()
plt.xticks(rotation=45)

# Plot 9: Prediction Accuracy Over Time
ax9 = plt.subplot(3, 3, 9)
correct_predictions = (results[best_model_name]['y_pred'] == y_test).astype(int)
window = 20
rolling_acc = pd.Series(correct_predictions).rolling(window=window).mean()
plt.plot(rolling_acc, linewidth=2)
plt.axhline(y=0.5, color='r', linestyle='--', label='Random (50%)')
plt.xlabel('Test Sample')
plt.ylabel('Accuracy')
plt.title(f'Rolling Accuracy ({window}-day window)', fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('stock_movement_classifier_analysis.png', dpi=300, bbox_inches='tight')
print("‚úì Visualization saved as 'stock_movement_classifier_analysis.png'")
plt.show()

# ============================================================================
# STEP 12: TRADING SIMULATION
# ============================================================================
print("\n" + "="*70)
print("[STEP 12] Backtesting Trading Strategy...")

# Simple strategy: Buy when predicted UP, Sell/Hold when predicted DOWN
initial_capital = 10000
capital = initial_capital
shares = 0
trades = []

test_data = df.iloc[split_idx:].reset_index(drop=True)
predictions_full = results[best_model_name]['y_pred']

for i in range(len(predictions_full)-1):
    current_price = test_data.iloc[i]['Close']
    next_price = test_data.iloc[i+1]['Close']
    prediction = predictions_full[i]
    
    # Buy signal
    if prediction == 1 and shares == 0:
        shares = capital / current_price
        capital = 0
        trades.append(('BUY', current_price, shares))
    
    # Sell signal
    elif prediction == 0 and shares > 0:
        capital = shares * current_price
        trades.append(('SELL', current_price, shares))
        shares = 0

# Final liquidation
if shares > 0:
    final_price = test_data.iloc[-1]['Close']
    capital = shares * final_price
    trades.append(('SELL', final_price, shares))
    shares = 0

final_value = capital
total_return = ((final_value - initial_capital) / initial_capital) * 100
buy_hold_return = ((test_data.iloc[-1]['Close'] - test_data.iloc[0]['Close']) / 
                   test_data.iloc[0]['Close']) * 100

print(f"\nTrading Results:")
print(f"  Initial Capital:        ${initial_capital:,.2f}")
print(f"  Final Portfolio Value:  ${final_value:,.2f}")
print(f"  Total Return:           {total_return:,.2f}%")
print(f"  Buy & Hold Return:      {buy_hold_return:,.2f}%")
print(f"  Outperformance:         {total_return - buy_hold_return:,.2f}%")
print(f"  Total Trades:           {len(trades)}")

# ============================================================================
# STEP 13: SUMMARY AND RECOMMENDATIONS
# ============================================================================
print("\n" + "="*70)
print("[STEP 13] Summary and Recommendations")
print("="*70)

print("\nüìä PROJECT SUMMARY:")
print(f"  ‚Ä¢ Total trading days analyzed: {len(df)}")
print(f"  ‚Ä¢ Features engineered: {len(feature_columns)}")
print(f"  ‚Ä¢ Best model: {best_model_name}")
print(f"  ‚Ä¢ Prediction accuracy: {results[best_model_name]['test_acc']*100:.2f}%")
print(f"  ‚Ä¢ ROC-AUC Score: {results[best_model_name]['roc_auc']:.4f}")

print("\nüí° KEY INSIGHTS:")
if best_model_name in ['Random Forest', 'Gradient Boosting']:
    top_3_features = feature_importance.head(3)['Feature'].tolist()
    print(f"  1. Most predictive features: {', '.join(top_3_features)}")
print(f"  2. Model performs {'better' if total_return > buy_hold_return else 'worse'} than buy-and-hold strategy")
print(f"  3. Precision: {results[best_model_name]['precision']:.2f} (reliability of UP predictions)")
print(f"  4. Recall: {results[best_model_name]['recall']:.2f} (ability to catch all UP movements)")

print("\nüéØ RECOMMENDATIONS:")
print("  ‚Ä¢ Use ensemble methods (Random Forest/Gradient Boosting) for best results")
print("  ‚Ä¢ Monitor RSI and MACD indicators - they show strong predictive power")
print("  ‚Ä¢ Consider adding more features: sentiment, news, macro indicators")
print("  ‚Ä¢ Implement risk management: stop-loss, position sizing")
print("  ‚Ä¢ Retrain model regularly with fresh data (monthly recommended)")
print("  ‚Ä¢ Combine with fundamental analysis for better decisions")
print("  ‚Ä¢ Test on multiple stocks before live trading")

print("\n‚ö†Ô∏è  DISCLAIMER:")
print("  This is an educational project. Past performance does not guarantee")
print("  future results. Always conduct thorough research and risk assessment")
print("  before making real trading decisions.")

print("\n" + "="*70)
print("PROJECT COMPLETED SUCCESSFULLY!")
print("="*70)
print("\n‚úÖ You now have a working ML-based stock movement classifier!")
print("‚úÖ Model saved and ready for predictions")
print("‚úÖ Use this as a foundation for more advanced strategies")
