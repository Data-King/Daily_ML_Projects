# A Neural Network from Scratch 

"""
A Logistic Regressor


The simplest neural network is a logistic regressor. Logistic regression takes in values of any range but out puts only values between zero anda one. 
There are many applications for logistic regressors. One example use case is to predict the likeihood of a homeowner to defauly on a mortage. We might take all 
kinds of values into account to predict the likelihood of defauly, the debtor's salary, whether she has a car, the security of her job, etc, but the likelihood
will always be a valye between zero and one. Even the worst debtor ever cannot have a default likelihood above 100% and the go below 0%.


We will use a library called numpy which enbales easy and fast matric operations in Python. To ensure we get the same result in all of our experiments,
we have to set a random seed.

"""

import numpy as np
np.random.seed(1)

# Since our dataset is quite small, we define it manually as numpy matrices


X = np.array([[0,1,0],
              [1,0,0],
              [1,1,1],
              [0,1,1]])

y = np.array([[0,1,1,0]]).T

"""
To compute the output of the regressor, we must first do a linear step. We compute the dor product of
the input X and the weights W. This is the same as multiplying each value of X with its weight and then
taking the sum. To this number, we add the bias. Afterwards, we do a nonlinear step. In the nonlinear
step, we run the linear intermediate product z throught an activation function, in this case, the sigmoid 
function. The sigmoid function squishes input values to outputs between zero and one. 



"""

# We can define the sigmoid activation function as a Python function:

def sigmoid(x):
    return 1/(1+np.exp(-x))

"""
So far, so good. Now we need to initialize W. In this case, we actaully know already which values W
should have. But we cannot know for other problems where we do not know the function yet. So, we have to assign weights randomly. 
The weights are usually assigned randomly with a mean of zero. The bias is usually set to zero by default.
Numpys random function excepts to recieve the shape of the random matrix to be passed on as a tuple so random((3,1)) creates a 3 by 1 matrix.
By default, the random values generated are between zero and one, with a mean of 0.5 and standard deviation of 0.5.
We want the random valyes to have a mean of 0 and a standard deviation of 1, so we first multiply the valyes generated by 2 and then subtract 1.


"""




W = 2*np.random.random((3,1)) -1 
b = 0

# Now that all variables are set, we can do the linear step:

z = X.dot(W) + b 

A = sigmoid(z)


# If we print out A now, we get the following output:
print(A)



"""
This looks nothing like our desired output y at allQ Clearly, our regressor is representing some function,
but it is quite far away from the function we want. To better approximate our desired function,
we have to tweak the weights W and the bias b to get better results. 

In this case, our problem is a binary classification problem, we will use the binary cross entropy loss:

Google Imgage and go through step by step on the equation.


"""


# In Python this loss function is implemented as follows:


def bce_loss(y,y_hat):
    N = y.shape[0]
    loss = -1/N * np.sum((y*np.log(y_hat) + (1 - y)*np.log(1-y_hat)))
    return loss

bce_loss(y,A)


"""
Backpropagation

To update the parameters, we need to calculate the derivative of the loss function with respect to the
weights and biases. Ig you imgaine the parameters of our models like the geo coordinates in our 
mountain analogy, caculating the loss derivative with respect to a parameter is like checking the 
mountain slope in the direction north to see wheater you should go north or south.


Note: to keep things simple, we refer to the derivative of the loss func to any variable as dvariable.
For example we write the derivative of the loss function with respect to the weights as dW.

To calculate the gradient with respect to different parameters of our model, we can make use of the chain rule as:


(f(g(x)))=g(x)*f(g(x))
Sometimes also written as:
dydx=dydududx

What the chain rule basically says is that if you want to take the derivative throught a number of nested
functions you multiply the derivative of the inner function with the derivative of the outer function. 
This is useful since neural networks, and our logistic regressor, are nested functions. The input goes though the linear step, a function of input, weights and biases. 
The output of the linear step, z goes throught the activation function. 

So when we compute the loss derivative with respect to weights and bias, we first compute the loss derivative with respect to the output of the linear step z,
 and use it to compute the dW. In code looks like this:


"""
N = y.shape[0]

dz = (A - y)

dW = 1/N * np.dot(X.T,dz)

db = 1/N * np.sum(dz,axis=0,keepdims=True)



"""
Parameter updates
Now we have the gradients, how do we improve our model? Or, to stay with our mountain analogy,
now that we know that the mountain goes up in the North direction and up in the East direction,
where do we go? To the South and to the West of course! MAthematically speaking, we go in the opposite
direction than the gradient. If the gradient is positive with respect to a parameter, speak the slope is
upward, we reduce the parameter. If it is negative, speak downward sloping, we increase it. When our
sloper is steeper, we move our gradient more.


The update rule for a paramter p then go like:
p = p a * dp

where p is a model parameter(either a weight or a bias), dp is the loss derivative with respect to p and a is the learning rate.
The learning rate is something like the gas pedal in a car. It sets how much we want to apply the gradient updates.
IT is one of those hyper parameters that we have to set manually. 


"""


# Randomly initialize the weights
W = 2*np.random.random((3,1)) - 1
b = 0

# Set the learning rate alpha to 1
alpha = 1

# We will train for 20 epochs
epochs = 20

# Count the number of training examples we have (4)
N = y.shape[0]


# In the loop below we do multiple forward and backward passes and apply the gradient descent update rule

losses = []
for i in range(epochs):
    # Do the linear step
    z = X.dot(W) + b

    # Do the non linear step
    A = sigmoid(z)

    # Calculate the loss
    loss = bce_loss(y,A)

    # Keep track of the loss
    print('Epoch:',i,'Loss:',loss)
    losses.append(loss)


    # Back propagate
    dz = (A - y)

# ...caculate loss derivative with respect to weights
dW = 1/N * np.dot(X.T,dz)


# ...calculate loss derivative with respect to bias
db = 1/N * np.sum(dz,axis=0,keepdims=True)


# Update parameters
W -= alpha * dW
b -= alpha * db

import matplotlib.pyplot as plt
plt.plot(losses)
plt.xlabel('epoch')
plt.ylabel('losses')
plt.show()
#fig.savefig('loss.jpg')


"""
A deeper network

We established earlier that in  order to approximate more complex functions, we need bigger,
deeper networks. Creating a deeper networks works by stacking layers on top of each pther.

In this section we will build a 2 layer neural network

The input gets multipled with the first set of weights W1, producing an intermediate product z1, and 
then run throught an activation function to produce the first layers activations A1. These activations
then get multipled with a second layer of weights of W2, prdocuing an intermediate product z2 which
gets run through a second activation function which produces the ourpur A2 of our neural net.

"""


# Package imports
# Matplotlib is a matlab like plotting library
import matplotlib
import matplotlib.pyplot as plt
# Numpy handles matrix operations
import numpy as np
# SciKitLearn is a useful machine learning utilities library
import sklearn
# The sklearn dataset module helps generating datasets
import sklearn.datasets
import sklearn.linear_model



# Display plots inline and change default figure size

matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)



# Just some helper functions we moved over from the last chapter
# sigmoid function
def sigmoid(x):
    '''
    Calculates the sigmoid activation of a given input x
    See: https://en.wikipedia.org/wiki/Sigmoid_function
    '''
    return 1/(1+np.exp(-x))

#Log Loss function
def bce_loss(y,y_hat):
    '''
    Calculates the logistic loss between a prediction y_hat and the labels y
    See: http://wiki.fast.ai/index.php/Log_Loss

    We need to clip values that get too close to zero to avoid zeroing out. 
    Zeroing out is when a number gets so small that the computer replaces it with 0.
    Therefore, we clip numbers to a minimum value.
    '''
    minval = 0.000000000001
    N = y.shape[0]
    l = -1/N * np.sum(y * np.log(y_hat.clip(min=minval)) + (1-y) * np.log((1-y_hat).clip(min=minval)))
    return l

# Log loss derivative
def bce_loss_derivative(y,y_hat):
    '''
    Calculates the gradient (derivative) of the log loss between point y and y_hat
    See: https://stats.stackexchange.com/questions/219241/gradient-for-logistic-loss-function
    '''
    return (y_hat-y)




def forward_prop(model,a0):
    '''
    Forward propagates through the model, stores results in cache.
    See: https://stats.stackexchange.com/questions/147954/neural-network-forward-propagation
    A0 is the activation at layer zero, it is the same as X
    '''


    # Load parameters from model
    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']

    # Linear step
    z1 = a0.dot(W1) + b1


    # First activation function
    a1 = np.tanh(z1)

    # Second linear step

    z2 = a1.dot(W2) + b2

    # Second activation function
    a2 = sigmoid(z2)
    cache = {'a0':a0, 'z1':z1, 'a1':a1, 'z1':z1, 'a2':a2}
    return cache

def tanh_derivative(x):
    '''
    Calculates the derivative of the tanh function that is used as the first activation function
    See: https://socratic.org/questions/what-is-the-derivative-of-tanh-x
    '''
    return (1 - np.power(x, 2))



def backward_prop(model,cache,y):
    '''
    Backward propagates through the model to calculate gradients.
    Stores gradients in grads dictionary.
    See: https://en.wikipedia.org/wiki/Backpropagation
    '''
    # Load parameters from model
    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']
    
    # Load forward propagation results
    a0,a1, a2 = cache['a0'],cache['a1'],cache['a2']
    
    # Backpropagation
    # Calculate loss derivative with respect to output
    dz2 = bce_loss_derivative(y=y,y_hat=a2)
    
    # Calculate loss derivative with respect to second layer weights
    dW2 = (a1.T).dot(dz2)
    
    # Calculate loss derivative with respect to second layer bias
    db2 = np.sum(dz2, axis=0, keepdims=True)
    
    # Calculate loss derivative with respect to first layer
    dz1 = dz2.dot(W2.T) * tanh_derivative(a1)
    
    # Calculate loss derivative with respect to first layer weights
    dW1 = np.dot(a0.T, dz1)
    
    # Calculate loss derivative with respect to first layer bias
    db1 = np.sum(dz1, axis=0)
    
    # Store gradients
    grads = {'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}
    return grads


# Helper function to plot a decision boundary.
# If you don't fully understand this function don't worry, it just generates the contour plot below.
def plot_decision_boundary(pred_func):
    # Set min and max values and give it some padding
    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # Predict the function value for the whole gid
    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # Plot the contour and training examples
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    plt.scatter(X[:, 0], X[:, 1], c=y.flatten(), cmap=plt.cm.Spectral)




# Generate a dataset and plot it
np.random.seed(0)
X, y = sklearn.datasets.make_moons(200, noise=0.15)
y = y.reshape(200,1)
plt.scatter(X[:,0], X[:,1], s=40, c=y.flatten(), cmap=plt.cm.Spectral)



def predict(model, x):
    '''
    Predicts y_hat as 1 or 0 for a given input X
    '''
    # Do forward pass
    c = forward_prop(model,x)
    #get y_hat
    y_hat = c['a2']
    
    # Turn values to either 1 or 0
    y_hat[y_hat > 0.5] = 1
    y_hat[y_hat < 0.5] = 0
    return y_hat



def calc_accuracy(model,x,y):
    '''
    Calculates the accuracy of the model given an input x and a correct output y.
    The accuracy is the percentage of examples our model classified correctly
    '''
    # Get total number of examples
    m = y.shape[0]
    # Do a prediction with the model
    pred = predict(model,x)
    # Ensure prediction and truth vector y have the same shape
    pred = pred.reshape(y.shape)
    # Calculate the number of wrong examples
    error = np.sum(np.abs(pred-y))
    # Calculate accuracy
    return (m - error)/m * 100




def calc_accuracy(model,x,y):
    '''
    Calculates the accuracy of the model given an input x and a correct output y.
    The accuracy is the percentage of examples our model classified correctly
    '''
    # Get total number of examples
    m = y.shape[0]
    # Do a prediction with the model
    pred = predict(model,x)
    # Ensure prediction and truth vector y have the same shape
    pred = pred.reshape(y.shape)
    # Calculate the number of wrong examples
    error = np.sum(np.abs(pred-y))
    # Calculate accuracy
    return (m - error)/m * 100




def calc_accuracy(model,x,y):
    '''
    Calculates the accuracy of the model given an input x and a correct output y.
    The accuracy is the percentage of examples our model classified correctly
    '''
    # Get total number of examples
    m = y.shape[0]
    # Do a prediction with the model
    pred = predict(model,x)
    # Ensure prediction and truth vector y have the same shape
    pred = pred.reshape(y.shape)
    # Calculate the number of wrong examples
    error = np.sum(np.abs(pred-y))
    # Calculate accuracy
    return (m - error)/m * 100




def initialize_parameters(nn_input_dim,nn_hdim,nn_output_dim):
    '''
    Initializes weights with random number between -1 and 1
    Initializes bias with 0
    Assigns weights and parameters to model
    '''
    # First layer weights
    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1
    
    # First layer bias
    b1 = np.zeros((1, nn_hdim))
    
    # Second layer weights
    W2 = 2 * np.random.randn(nn_hdim, nn_output_dim) - 1
    
    # Second layer bias
    b2 = np.zeros((1, nn_output_dim))
    
    # Package and return model
    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}
    return model




def update_parameters(model,grads,learning_rate):
    '''
    Updates parameters accoarding to gradient descent algorithm
    See: https://en.wikipedia.org/wiki/Gradient_descent
    '''
    # Load parameters
    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']
    
    # Update parameters
    W1 -= learning_rate * grads['dW1']
    b1 -= learning_rate * grads['db1']
    W2 -= learning_rate * grads['dW2']
    b2 -= learning_rate * grads['db2']
    
    # Store and return parameters
    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}
    return model



def train(model,X_,y_,learning_rate, num_passes=20000, print_loss=False):
    # Gradient descent. For each batch...
    for i in range(0, num_passes):

        # Forward propagation
        cache = forward_prop(model,X_)
        #a1, probs = cache['a1'],cache['a2']
        # Backpropagation
        
        grads = backward_prop(model,cache,y)
        # Gradient descent parameter update
        # Assign new parameters to the model
        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)
    
        # Pring loss & accuracy every 100 iterations
        if print_loss and i % 100 == 0:
            y_hat = cache['a2']
            print('Loss after iteration',i,':',bce_loss(y,y_hat))
            print('Accuracy after iteration',i,':',calc_accuracy(model,X_,y_),'%')
    
    return model



"""
Little noise and a good hidden size


In this section, we will fit a model with a good hidden layer size to data with little noise

"""


# Hyper parameters
hiden_layer_size = 3
# I picked this value because it showed good results in my experiments
learning_rate = 0.01
# Initialize the parameters to random values. We need to learn these.
np.random.seed(0)
# This is what we return at the end
model = initialize_parameters(nn_input_dim=2, nn_hdim= hiden_layer_size, nn_output_dim= 1)
model = train(model,X,y,learning_rate=learning_rate,num_passes=1000,print_loss=True)
# Plot the decision boundary
plot_decision_boundary(lambda x: predict(model,x))
plt.title("Decision Boundary for hidden layer size 3")
# Now with more noise
# Generate a dataset and plot it
np.random.seed(0)
# The data generator alows us to regulate the noise level
X, y = sklearn.datasets.make_moons(200, noise=0.3)
y = y.reshape(200,1)
plt.scatter(X[:,0], X[:,1], s=40, c=y.flatten(), cmap=plt.cm.Spectral)





"""
Too small hidden size



In this section, the hidden layer size is 1, which is too small. The data also has more noise.

"""



# Hyper parameters
hiden_layer_size = 1
# I picked this value because it showed good results in my experiments
learning_rate = 0.01

# Initialize the parameters to random values. We need to learn these.
np.random.seed(0)
# This is what we return at the end
model = initialize_parameters(nn_input_dim=2, nn_hdim= hiden_layer_size, nn_output_dim= 1)
model = train(model,X,y,learning_rate=learning_rate,num_passes=1000,print_loss=True)
# Plot the decision boundary
plot_decision_boundary(lambda x: predict(model,x))
plt.title("Decision Boundary for hidden layer size 1")



"""
Too large hidden layer size


In this section, the hidden layer size is too large and the model fits the noise.

"""



# Hyper parameters
hiden_layer_size = 500
# I picked this value because it showed good results in my experiments
learning_rate = 0.01

# Initialize the parameters to random values. We need to learn these.
np.random.seed(0)
# This is what we return at the end
model = initialize_parameters(nn_input_dim=2, nn_hdim= hiden_layer_size, nn_output_dim= 1)
model = train(model,X,y,learning_rate=learning_rate,num_passes=1000,print_loss=True)
# Plot the decision boundary
# This might take a little while as our model is very big now
plot_decision_boundary(lambda x: predict(model,x))
plt.title("Decision Boundary for hidden layer size 500")



"""

Keras



In this section we will build the same model with the Keras Sequential API

"""



# Generate a dataset and plot it
np.random.seed(0)
X, y = sklearn.datasets.make_moons(200, noise=0.15)
y = y.reshape(200,1)



"""
Importing Keras
When importing Keras, we usually just import the modules we will use. In this case, we need two types of layers:
 The Dense layer is the plain layer which we have gotten to know in this chapter. The Activation layer allows us to add an activation function. We can import them like this:

from keras.layers import Dense, Activation
Keras offers two kinds of ways to build models. 

The sequential and the functional API. The sequential API is easier to use and allows more rapid building of models so we will use it in most of the book.
 In later chapters we will take a look at the functional API as well. We can access the sequential API like this:

from keras.models import Sequential


"""


from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.models import Sequential




"""
A two layer model in Keras
Building a neural network in the sequential API works as follows:

Stacking layers
First, we create an empty sequential model with no layers:

model = Sequential()
Then we can add layers to this model just like stacking a layer cake with model.add(). 
For the first layer, we have to specify the input dimensions of the layer.
 In our case, the data has two features, the coordinates of the point. We can add a hidden layer with hidden layer size 3 like this:

model.add(Dense(3,input_dim=2))
Note how we nest the functions: Inside model.add() we specify the Dense layer.
 The positional argument is the size of the layer. This Dense layer now only does the linear step. To add a tanh activation function, we call:

model.add(Activation('tanh'))
We add the linear step and the activation function of the output layer in the same way:

model.add(Dense(1))
model.add(Activation('sigmoid'))

"""



model = Sequential()
model.add(Dense(3,input_dim=2))
model.add(Activation('tanh'))
model.add(Dense(1))
model.add(Activation('sigmoid'))


"""
Compiling the model

Before we can start training the model we have to specify how exactly we want to train the model.
Most importantly we need to specify which optimizer and which loss function we want to use. 
The simple optimizer we have used so far is called 'Stochastic Gradient Descent' or SGD, for more optimizers, see chapter 2.
The loss function we use for this binary classification problem is called 'binary crossentropy'.
We can also specify which metrics we want to track during training. In our case, accuracy would be interesting to track, or just 'acc' to keep it short.


"""


model.compile(optimizer='sgd',loss='binary_crossentropy',metrics=['acc'])
model.summary()
# Training the model
# 
# Now we are ready to run the training process:

history = model.fit(X,y,epochs=900)